{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pizza2vec build ipynb\n",
    "### Karl Konz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the Yelp data set, you will need to register for the data set and choose whether to download JSON or data in SQL format.\n",
    "The data is slightly smaller in JSON, so I chose to opt for less overhead. Below we use the codecs and os packages to read in the business records and write to disk. This is a lean version of the Patrick Harrison tutorial, with the addition of creating a html bokeh plot for later embedding and clustering to better interpret the t-sne reduction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\": \"YDf95gJZaq05wvo7hTQbbQ\", \"name\": \"Richmond Town Square\", \"neighborhood\": \"\", \"address\": \"691 Richmond Rd\", \"city\": \"Richmond Heights\", \"state\": \"OH\", \"postal_code\": \"44143\", \"latitude\": 41.5417162, \"longitude\": -81.4931165, \"stars\": 2.0, \"review_count\": 17, \"is_open\": 1, \"attributes\": {\"RestaurantsPriceRange2\": 2, \"BusinessParking\": {\"garage\": false, \"street\": false, \"validated\": false, \"lot\": true, \"valet\": false}, \"BikeParking\": true, \"WheelchairAccessible\": true}, \"categories\": [\"Shopping\", \"Shopping Centers\"], \"hours\": {\"Monday\": \"10:00-21:00\", \"Tuesday\": \"10:00-21:00\", \"Friday\": \"10:00-21:00\", \"Wednesday\": \"10:00-21:00\", \"Thursday\": \"10:00-21:00\", \"Sunday\": \"11:00-18:00\", \"Saturday\": \"10:00-21:00\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the desired packages\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "# Create a data_directory path object.\n",
    "data_directory = os.path.join('..', 'data')\n",
    "\n",
    "# Join the path where the data is located \n",
    "# to the name of the yelp business dataset\n",
    "businesses_filepath = os.path.join(data_directory,\n",
    "                                   'business.json')\n",
    "\n",
    "# Using codecs, read the data, and print the first business record.\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will do the same for the review records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"VfBHSwC5Vz_pbFluy07i9Q\",\"user_id\":\"cjpdDjZyprfyDG3RlkVG3w\",\"business_id\":\"uYHaNptLzDLoV_JZ_MuzUA\",\"stars\":5,\"date\":\"2016-07-12\",\"text\":\"My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\n\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\n\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a review file path object\n",
    "review_json_filepath = os.path.join(data_directory,\n",
    "                                    'review.json')\n",
    "\n",
    "# iterate through the review data and print the first review record.\n",
    "with codecs.open(review_json_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we read all of the restaurants in the Yelp data set and print the number of restaurants included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51,613 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# open the businesses file\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # if this business is not a restaurant, skip to the next one\n",
    "        if u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "            \n",
    "        # add the restaurant business id to our restaurant_ids set\n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "\n",
    "# turn restaurant_ids into a frozenset, as we don't need to change it anymore\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# print the number of unique restaurant ids in the dataset\n",
    "print('{:,}'.format(len(restaurant_ids)), u'restaurants in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': '540 Marks St',\n",
       " 'attributes': {'BikeParking': True,\n",
       "  'BusinessAcceptsCreditCards': True,\n",
       "  'BusinessParking': {'garage': False,\n",
       "   'lot': True,\n",
       "   'street': False,\n",
       "   'valet': False,\n",
       "   'validated': False},\n",
       "  'RestaurantsPriceRange2': 1,\n",
       "  'WheelchairAccessible': True},\n",
       " 'business_id': 'scMIE4jyGp7FkWrMKAgjxA',\n",
       " 'categories': ['Fashion',\n",
       "  'Shopping',\n",
       "  'Food',\n",
       "  'Department Stores',\n",
       "  'Mobile Phones',\n",
       "  'Grocery'],\n",
       " 'city': 'Henderson',\n",
       " 'hours': {},\n",
       " 'is_open': 0,\n",
       " 'latitude': 36.0607079,\n",
       " 'longitude': -115.0332406,\n",
       " 'name': 'Walmart',\n",
       " 'neighborhood': '',\n",
       " 'postal_code': '89014',\n",
       " 'review_count': 42,\n",
       " 'stars': 2.5,\n",
       " 'state': 'NV'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new file that contains only the text from reviews about restaurants, with one review per line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an intermediate directory path object\n",
    "intermediate_directory = os.path.join('..', 'intermediate')\n",
    "\n",
    "# Create the text file object for all the reviews.\n",
    "review_txt_filepath = os.path.join(intermediate_directory,\n",
    "                                   'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write the reivews to disk. Then print out the number of reviews and the length in time to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 193,145 restaurant reviews in the txt file.\n",
      "CPU times: user 4 s, sys: 38.7 ms, total: 4.04 s\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(review_txt_filepath, 'wb', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        # open the existing review json file\n",
    "        with codecs.open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "            # loop through all reviews in the existing file and convert to dict\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                # if this review is not about a restaurant, skip to the next one\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "                \n",
    "                if 'pizza' not in review[u'text']:\n",
    "                    continue\n",
    "\n",
    "                # write the restaurant review as a line in the new file\n",
    "                # escape newline characters in the original review text\n",
    "                review_txt_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "\n",
    "    print(u'''Text from {:,} restaurant reviews\n",
    "              written to the new txt file.'''.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(review_txt_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print(u'Text from {:,} restaurant reviews in the txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will segement text of complete reviews into sentences and normalize the text.\n",
    "Read in the spacy, pandas, gensim, natural language tool kit, and itertools packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spacy object, and a unigram sentence path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create an english spacy object\n",
    "nlp = spacy.load('en')\n",
    "# create a path file for the unigram sentences.\n",
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create some helper functions for tokenization, new line formating, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    # For each review apply spacy nlp.pipe in 10,000 batch size with 4 threads.\n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        # apply lemma_ for each of the reviews parsed, remove punct_space records.\n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next apply the lemmatized_sentence_corpus helper described above from the review text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up the path objects to now create bigrams representations of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 215 ms, total: 2.05 s\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "# unigram sentences file object\n",
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "# path object for bigram model\n",
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "    \n",
    "    # apply the Phrases call to the unigram_sentences object\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    # save the bigram model\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "# create a bigram text output file path object\n",
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'bigram_sentences_all.txt')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "    # iterate through the bigram sentences\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        # for each of the unigram sentences\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            # format the bigram sentences\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            # write the bigram sentences\n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next create a trigram model from the bigram sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 s, sys: 165 ms, total: 2.18 s\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "# create an object of the bigram sentences\n",
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "# create a path object for the trigram model\n",
    "trigram_model_filepath = os.path.join(intermediate_directory,\n",
    "                                      'trigram_model_all')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "    # create a trigram model\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    # save the trigram model\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, write the trigram sentences to a text file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "# create the path object for the trigrams\n",
    "trigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'trigram_sentences_all.txt')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "    # open the trigram sentences.\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        # iterate through the bigram sentences\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            # format the bigram model objects\n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            # write the trigram sentences\n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we are going to apply the helper functions to the trigram sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.44 ms, sys: 8.81 ms, total: 15.3 ms\n",
      "Wall time: 27.6 ms\n"
     ]
    }
   ],
   "source": [
    "# create a trigrams object\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "# create the path object for the trigrams review text file.\n",
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')\n",
    "# create the stopwords object \n",
    "STOPWORDS = stopwords.words()\n",
    "\n",
    "%%time\n",
    "#nlp = spacy.load('en')\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "\n",
    "if 0 == 1:\n",
    "    \n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=10000):#, n_threads=4): nlp pipe does not support mThreading?\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in STOPWORDS\n",
    "                              #nlp.Defaults.STOPWORDS Not working\n",
    "                             ]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we create the trigram dictionary text path object. We also will remove any tokens that are very rare or too common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.8 ms, sys: 3.19 ms, total: 13 ms\n",
      "Wall time: 102 ms\n"
     ]
    }
   ],
   "source": [
    "# create the trigram dictionary text path\n",
    "trigram_dictionary_filepath = os.path.join(intermediate_directory,\n",
    "                                           'trigram_dict_all.dict')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "    # save the trigrams dictionary model\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next create a trigram bag of words generator helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    # use the doc2bow function for each review instance in the file path.\n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create the trigram review bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 ms, sys: 6.38 ms, total: 30.2 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "# create the trigram reviews object\n",
    "trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "# create the mm file path for the trigram bag of words file path\n",
    "trigram_bow_filepath = os.path.join(intermediate_directory,\n",
    "                                    'trigram_bow_corpus_all.mm')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_reviews_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we will create latent direlecht allocation model for topic and word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the needed packages\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Word2Vec\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import _pickle as pickle\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.6 ms, sys: 27.6 ms, total: 105 ms\n",
      "Wall time: 378 ms\n"
     ]
    }
   ],
   "source": [
    "# create the path object for the lda model all for term frequencies.\n",
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        # create an lda model\n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3)\n",
    "    # save the lda model\n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a helper function to exploe the to number of term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the related terms to a given topic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "gelato               0.023\n",
      "coffee               0.018\n",
      "get                  0.011\n",
      "go                   0.010\n",
      "time                 0.010\n",
      "much                 0.009\n",
      "one                  0.008\n",
      "food                 0.006\n",
      "terroni              0.006\n",
      "try                  0.006\n",
      "like                 0.006\n",
      "little               0.005\n",
      "really               0.005\n",
      "back                 0.005\n",
      "make                 0.005\n",
      "cafe                 0.005\n",
      "even                 0.005\n",
      "well                 0.004\n",
      "would                0.004\n",
      "burrito              0.004\n",
      "know                 0.004\n",
      "pastry               0.004\n",
      "open                 0.004\n",
      "scoop                0.004\n",
      "think                0.003\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next assign topic names to each of the 50 topics for the lda model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "# Setting topics to the top 2 similar responses\n",
    "topic_names = {0: u'bar beer',\n",
    "               1: u'great service',\n",
    "               2: u'wing order',\n",
    "               3: u'cheese crust',\n",
    "               4: u'much order',\n",
    "               5: u'$ get',\n",
    "               6: u'great food',\n",
    "               7: u'hoagie well',\n",
    "               8: u'time party',\n",
    "               9: u'poutine like',\n",
    "               10: u'vegan great',\n",
    "               11: u'kid food',\n",
    "               12: u'bianco raw',\n",
    "               13: u'make one',\n",
    "               14: u'beer happy_hour',\n",
    "               15: u'get like',\n",
    "               16: u'topping make',\n",
    "               17: u'go like',\n",
    "               18: u'much get',\n",
    "               19: u'food italian',\n",
    "               20: u'owner restaurant',\n",
    "               21: u'sandwich like',\n",
    "               22: u'love salad',\n",
    "               23: u'buffet breakfast',\n",
    "               24: u'wait order',\n",
    "               25: u'go great',\n",
    "               26: u'rosa get',\n",
    "               27: u'ask say',\n",
    "               28: u'bianco_pizzaria menu',\n",
    "               29: u'food restaurant',\n",
    "               30: u'deep_dish cheese',\n",
    "               31: u'slice burger',\n",
    "               32: u'table one',\n",
    "               33: u'salad wine',\n",
    "               34: u'much pittsburgh',\n",
    "               35: u'sauce crust',\n",
    "               36: u'order call',\n",
    "               37: u'great love',\n",
    "               38: u'menu much',\n",
    "               39: u'order make',\n",
    "               40: u'get say',\n",
    "               41: u'like really',\n",
    "               42: u'time order',\n",
    "               43: u'food dish',\n",
    "               44: u'chicken buffet',\n",
    "               45: u'gyro well',\n",
    "               46: u'like much',\n",
    "               47: u'well go',\n",
    "               48: u'order sushi',\n",
    "               49: u'gelato coffee'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next create the topic names version of the trigram bag of words corpus for lda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 21s, sys: 963 ms, total: 10min 22s\n",
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "# create the topic_names pickle file path objects.\n",
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "\n",
    "# write the topic names\n",
    "with open(topic_names_filepath, 'wb') as f:\n",
    "    pickle.dump(topic_names, f)\n",
    "\n",
    "# create lda file path for pickle file\n",
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared.pkl')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 0:\n",
    "\n",
    "    # create lda trigram bow model with trigram dictionary\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus,\n",
    "                                              trigram_dictionary)\n",
    "\n",
    "    #with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    #    pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "#with open(LDAvis_data_filepath) as f:\n",
    "#    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions to get sample reviews, and for a lda description output generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_review(review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(review_txt_filepath),\n",
    "                          review_number, review_number+1))[0]\n",
    "\n",
    "def lda_description(review_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    accept the original text of a review and (1) parse it with spaCy,\n",
    "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n",
    "    representation, (4) create an LDA representation, and\n",
    "    (5) print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the review text with spaCy\n",
    "    parsed_review = nlp(review_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "    # remove any remaining stopwords\n",
    "    trigram_review = [term for term in trigram_review\n",
    "                      if not term in STOPWORDS]\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    review_lda = lda[review_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    review_lda = sorted(review_lda, key=lambda topic_number_and_freq: -topic_number_and_freq[1])\n",
    "    \n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        print('{:25} {}'.format(topic_names[topic_number], round(freq, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 training epochs so far.\n",
      "CPU times: user 165 ms, sys: 31.7 ms, total: 196 ms\n",
      "Wall time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "# create a trigram sentences object\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "# create a word2vec file path object\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')\n",
    "\n",
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    food2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                        min_count=20, sg=1, workers=4)\n",
    "    # save the word2vec model\n",
    "    food2vec.save(word2vec_filepath)\n",
    "\n",
    "    # perform another 11 epochs of training\n",
    "    for i in range(1,12):\n",
    "        # train the word2vec model\n",
    "        food2vec.train(trigram_sentences, \n",
    "                       total_examples=food2vec.corpus_count, \n",
    "                       epochs=food2vec.iter)#, total_examples=corpus_count, epochs=iter)\n",
    "        # save the trained version of the model\n",
    "        food2vec.save(word2vec_filepath)\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print(u'{} training epochs so far.'.format(food2vec.train_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,752 terms in the food2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print(u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shipwreck_burger</th>\n",
       "      <td>-0.008992</td>\n",
       "      <td>0.069071</td>\n",
       "      <td>-0.044403</td>\n",
       "      <td>-0.066379</td>\n",
       "      <td>-0.110571</td>\n",
       "      <td>-0.051169</td>\n",
       "      <td>0.096304</td>\n",
       "      <td>0.020024</td>\n",
       "      <td>0.033676</td>\n",
       "      <td>-0.148641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116319</td>\n",
       "      <td>-0.141442</td>\n",
       "      <td>-0.031097</td>\n",
       "      <td>-0.092572</td>\n",
       "      <td>0.182790</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>-0.093378</td>\n",
       "      <td>0.032352</td>\n",
       "      <td>0.063388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fat_uncle_tony</th>\n",
       "      <td>-0.110202</td>\n",
       "      <td>0.124813</td>\n",
       "      <td>0.024812</td>\n",
       "      <td>-0.116159</td>\n",
       "      <td>0.239851</td>\n",
       "      <td>0.057880</td>\n",
       "      <td>0.018485</td>\n",
       "      <td>0.161749</td>\n",
       "      <td>0.024854</td>\n",
       "      <td>0.074692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045939</td>\n",
       "      <td>-0.074423</td>\n",
       "      <td>-0.024706</td>\n",
       "      <td>-0.006327</td>\n",
       "      <td>0.113073</td>\n",
       "      <td>0.184647</td>\n",
       "      <td>0.044923</td>\n",
       "      <td>-0.068010</td>\n",
       "      <td>0.037040</td>\n",
       "      <td>-0.089825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dianoia</th>\n",
       "      <td>-0.076524</td>\n",
       "      <td>0.178293</td>\n",
       "      <td>-0.033832</td>\n",
       "      <td>0.115640</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.158894</td>\n",
       "      <td>0.154938</td>\n",
       "      <td>0.072801</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>-0.077997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052969</td>\n",
       "      <td>-0.049967</td>\n",
       "      <td>0.075205</td>\n",
       "      <td>-0.054349</td>\n",
       "      <td>0.098447</td>\n",
       "      <td>0.025781</td>\n",
       "      <td>-0.023025</td>\n",
       "      <td>-0.035267</td>\n",
       "      <td>0.073016</td>\n",
       "      <td>-0.060536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lamb_chorizo</th>\n",
       "      <td>-0.115281</td>\n",
       "      <td>0.116881</td>\n",
       "      <td>0.059889</td>\n",
       "      <td>0.090417</td>\n",
       "      <td>0.036778</td>\n",
       "      <td>0.034917</td>\n",
       "      <td>-0.024799</td>\n",
       "      <td>0.172585</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>0.043193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080942</td>\n",
       "      <td>0.068433</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.045230</td>\n",
       "      <td>0.045128</td>\n",
       "      <td>0.104081</td>\n",
       "      <td>0.159248</td>\n",
       "      <td>-0.066578</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.064715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>organ_music</th>\n",
       "      <td>-0.049580</td>\n",
       "      <td>-0.123448</td>\n",
       "      <td>0.139327</td>\n",
       "      <td>0.040766</td>\n",
       "      <td>-0.093590</td>\n",
       "      <td>0.176509</td>\n",
       "      <td>-0.108355</td>\n",
       "      <td>-0.063220</td>\n",
       "      <td>-0.013871</td>\n",
       "      <td>0.020749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014698</td>\n",
       "      <td>0.006284</td>\n",
       "      <td>-0.097220</td>\n",
       "      <td>-0.056107</td>\n",
       "      <td>-0.158724</td>\n",
       "      <td>-0.071799</td>\n",
       "      <td>-0.026583</td>\n",
       "      <td>0.028074</td>\n",
       "      <td>-0.068363</td>\n",
       "      <td>-0.097564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secret_hallway</th>\n",
       "      <td>-0.011642</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>-0.015406</td>\n",
       "      <td>0.147396</td>\n",
       "      <td>-0.061920</td>\n",
       "      <td>-0.062190</td>\n",
       "      <td>-0.090432</td>\n",
       "      <td>0.142934</td>\n",
       "      <td>-0.017427</td>\n",
       "      <td>-0.092649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056522</td>\n",
       "      <td>-0.127429</td>\n",
       "      <td>-0.080508</td>\n",
       "      <td>0.077457</td>\n",
       "      <td>0.084034</td>\n",
       "      <td>-0.116520</td>\n",
       "      <td>-0.086455</td>\n",
       "      <td>0.104335</td>\n",
       "      <td>0.011970</td>\n",
       "      <td>0.142961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5am</th>\n",
       "      <td>0.064431</td>\n",
       "      <td>0.013835</td>\n",
       "      <td>-0.101313</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>-0.139770</td>\n",
       "      <td>0.105381</td>\n",
       "      <td>-0.216017</td>\n",
       "      <td>0.086563</td>\n",
       "      <td>-0.062952</td>\n",
       "      <td>-0.033686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128176</td>\n",
       "      <td>-0.069481</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.165028</td>\n",
       "      <td>0.023574</td>\n",
       "      <td>-0.016147</td>\n",
       "      <td>-0.039076</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>0.040436</td>\n",
       "      <td>-0.069485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apiecalypse</th>\n",
       "      <td>-0.189204</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>0.094419</td>\n",
       "      <td>-0.101405</td>\n",
       "      <td>-0.085885</td>\n",
       "      <td>0.050780</td>\n",
       "      <td>-0.029000</td>\n",
       "      <td>0.145084</td>\n",
       "      <td>0.056160</td>\n",
       "      <td>0.155907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077971</td>\n",
       "      <td>-0.122632</td>\n",
       "      <td>-0.172046</td>\n",
       "      <td>0.116918</td>\n",
       "      <td>0.071860</td>\n",
       "      <td>0.039033</td>\n",
       "      <td>-0.064204</td>\n",
       "      <td>0.038068</td>\n",
       "      <td>0.036973</td>\n",
       "      <td>-0.160624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jicama_fry</th>\n",
       "      <td>-0.040205</td>\n",
       "      <td>0.169634</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>0.082035</td>\n",
       "      <td>-0.132468</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.086396</td>\n",
       "      <td>0.129385</td>\n",
       "      <td>-0.026789</td>\n",
       "      <td>-0.144426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031000</td>\n",
       "      <td>0.045968</td>\n",
       "      <td>0.007623</td>\n",
       "      <td>-0.015678</td>\n",
       "      <td>0.237579</td>\n",
       "      <td>-0.151209</td>\n",
       "      <td>0.134013</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>0.083593</td>\n",
       "      <td>-0.109475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joanna</th>\n",
       "      <td>0.042130</td>\n",
       "      <td>0.051496</td>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.024183</td>\n",
       "      <td>0.220385</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>0.178014</td>\n",
       "      <td>0.093679</td>\n",
       "      <td>0.058013</td>\n",
       "      <td>-0.028825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057799</td>\n",
       "      <td>-0.059719</td>\n",
       "      <td>0.015825</td>\n",
       "      <td>-0.007224</td>\n",
       "      <td>-0.094543</td>\n",
       "      <td>-0.010705</td>\n",
       "      <td>0.086375</td>\n",
       "      <td>-0.033077</td>\n",
       "      <td>-0.097246</td>\n",
       "      <td>0.027874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emeril</th>\n",
       "      <td>-0.024246</td>\n",
       "      <td>0.017684</td>\n",
       "      <td>0.112363</td>\n",
       "      <td>0.161033</td>\n",
       "      <td>-0.062193</td>\n",
       "      <td>-0.009788</td>\n",
       "      <td>-0.028214</td>\n",
       "      <td>-0.112537</td>\n",
       "      <td>0.092924</td>\n",
       "      <td>-0.029265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094892</td>\n",
       "      <td>-0.101644</td>\n",
       "      <td>-0.060094</td>\n",
       "      <td>-0.104245</td>\n",
       "      <td>-0.165910</td>\n",
       "      <td>0.096416</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>-0.023789</td>\n",
       "      <td>0.059632</td>\n",
       "      <td>0.066779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoli</th>\n",
       "      <td>-0.020396</td>\n",
       "      <td>0.063914</td>\n",
       "      <td>0.085350</td>\n",
       "      <td>0.152904</td>\n",
       "      <td>-0.055640</td>\n",
       "      <td>-0.226780</td>\n",
       "      <td>0.077708</td>\n",
       "      <td>-0.023281</td>\n",
       "      <td>-0.050898</td>\n",
       "      <td>-0.034766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019899</td>\n",
       "      <td>0.064196</td>\n",
       "      <td>0.087739</td>\n",
       "      <td>-0.241405</td>\n",
       "      <td>0.078495</td>\n",
       "      <td>0.050972</td>\n",
       "      <td>-0.004864</td>\n",
       "      <td>-0.168966</td>\n",
       "      <td>0.044509</td>\n",
       "      <td>-0.019740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brimstone</th>\n",
       "      <td>-0.134403</td>\n",
       "      <td>-0.007808</td>\n",
       "      <td>-0.040262</td>\n",
       "      <td>-0.006577</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>-0.042600</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.194187</td>\n",
       "      <td>-0.017311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134744</td>\n",
       "      <td>-0.023471</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>-0.154001</td>\n",
       "      <td>0.062192</td>\n",
       "      <td>-0.058664</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>0.124316</td>\n",
       "      <td>-0.001063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soppressata_marmalade</th>\n",
       "      <td>-0.050193</td>\n",
       "      <td>-0.086515</td>\n",
       "      <td>-0.008821</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.137798</td>\n",
       "      <td>-0.040572</td>\n",
       "      <td>0.105973</td>\n",
       "      <td>0.090856</td>\n",
       "      <td>0.121990</td>\n",
       "      <td>0.039391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049613</td>\n",
       "      <td>-0.041060</td>\n",
       "      <td>-0.047666</td>\n",
       "      <td>0.039599</td>\n",
       "      <td>0.136480</td>\n",
       "      <td>0.116905</td>\n",
       "      <td>0.105467</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>0.002577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choc_chip</th>\n",
       "      <td>-0.073072</td>\n",
       "      <td>0.184384</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>-0.006679</td>\n",
       "      <td>-0.178735</td>\n",
       "      <td>0.076444</td>\n",
       "      <td>-0.057219</td>\n",
       "      <td>0.083627</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>-0.009538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017959</td>\n",
       "      <td>-0.076338</td>\n",
       "      <td>0.083766</td>\n",
       "      <td>-0.082891</td>\n",
       "      <td>0.121686</td>\n",
       "      <td>-0.084918</td>\n",
       "      <td>0.026835</td>\n",
       "      <td>0.121846</td>\n",
       "      <td>-0.075995</td>\n",
       "      <td>-0.016449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vero_bistro</th>\n",
       "      <td>-0.181221</td>\n",
       "      <td>0.074585</td>\n",
       "      <td>0.137863</td>\n",
       "      <td>0.047094</td>\n",
       "      <td>0.082426</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.060102</td>\n",
       "      <td>0.038484</td>\n",
       "      <td>0.026909</td>\n",
       "      <td>-0.043411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052806</td>\n",
       "      <td>-0.130720</td>\n",
       "      <td>-0.067285</td>\n",
       "      <td>-0.192226</td>\n",
       "      <td>-0.074968</td>\n",
       "      <td>0.046571</td>\n",
       "      <td>-0.123345</td>\n",
       "      <td>-0.035305</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>-0.130798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oberlin</th>\n",
       "      <td>-0.007305</td>\n",
       "      <td>-0.080906</td>\n",
       "      <td>0.074522</td>\n",
       "      <td>0.085458</td>\n",
       "      <td>-0.005709</td>\n",
       "      <td>0.020881</td>\n",
       "      <td>-0.109144</td>\n",
       "      <td>0.164599</td>\n",
       "      <td>0.102354</td>\n",
       "      <td>0.013921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038243</td>\n",
       "      <td>0.075031</td>\n",
       "      <td>-0.161663</td>\n",
       "      <td>-0.131113</td>\n",
       "      <td>-0.036827</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>-0.045671</td>\n",
       "      <td>-0.135558</td>\n",
       "      <td>-0.085684</td>\n",
       "      <td>0.090239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aria_hotel</th>\n",
       "      <td>0.015693</td>\n",
       "      <td>-0.047445</td>\n",
       "      <td>-0.054489</td>\n",
       "      <td>0.213119</td>\n",
       "      <td>-0.170043</td>\n",
       "      <td>0.127753</td>\n",
       "      <td>0.049336</td>\n",
       "      <td>0.123832</td>\n",
       "      <td>0.019755</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145236</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>-0.119692</td>\n",
       "      <td>0.038456</td>\n",
       "      <td>0.005336</td>\n",
       "      <td>-0.091374</td>\n",
       "      <td>-0.212037</td>\n",
       "      <td>0.067723</td>\n",
       "      <td>0.044053</td>\n",
       "      <td>0.114976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shady_park</th>\n",
       "      <td>-0.219032</td>\n",
       "      <td>-0.094213</td>\n",
       "      <td>-0.012360</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>-0.020752</td>\n",
       "      <td>0.094453</td>\n",
       "      <td>0.128042</td>\n",
       "      <td>0.022693</td>\n",
       "      <td>-0.065575</td>\n",
       "      <td>0.096917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046334</td>\n",
       "      <td>-0.016325</td>\n",
       "      <td>-0.133936</td>\n",
       "      <td>-0.037850</td>\n",
       "      <td>-0.050931</td>\n",
       "      <td>-0.033585</td>\n",
       "      <td>-0.102373</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>-0.023786</td>\n",
       "      <td>-0.087078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caprese_bruschetta</th>\n",
       "      <td>-0.011774</td>\n",
       "      <td>0.130502</td>\n",
       "      <td>0.120560</td>\n",
       "      <td>-0.169919</td>\n",
       "      <td>0.091245</td>\n",
       "      <td>0.138024</td>\n",
       "      <td>0.125142</td>\n",
       "      <td>0.253348</td>\n",
       "      <td>0.025978</td>\n",
       "      <td>0.086949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135232</td>\n",
       "      <td>-0.079342</td>\n",
       "      <td>0.093287</td>\n",
       "      <td>0.043018</td>\n",
       "      <td>0.260069</td>\n",
       "      <td>0.040450</td>\n",
       "      <td>-0.064958</td>\n",
       "      <td>-0.107730</td>\n",
       "      <td>-0.022220</td>\n",
       "      <td>0.026259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davinci</th>\n",
       "      <td>-0.038101</td>\n",
       "      <td>0.199305</td>\n",
       "      <td>0.014235</td>\n",
       "      <td>-0.183532</td>\n",
       "      <td>0.100110</td>\n",
       "      <td>-0.137999</td>\n",
       "      <td>0.191221</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.165447</td>\n",
       "      <td>0.122837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129809</td>\n",
       "      <td>-0.035600</td>\n",
       "      <td>-0.001983</td>\n",
       "      <td>0.072397</td>\n",
       "      <td>-0.013816</td>\n",
       "      <td>-0.004943</td>\n",
       "      <td>0.061289</td>\n",
       "      <td>-0.148010</td>\n",
       "      <td>0.101043</td>\n",
       "      <td>-0.200894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hor</th>\n",
       "      <td>0.133340</td>\n",
       "      <td>0.198932</td>\n",
       "      <td>-0.045366</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>-0.034952</td>\n",
       "      <td>0.165558</td>\n",
       "      <td>0.136696</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.066247</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.055166</td>\n",
       "      <td>-0.067630</td>\n",
       "      <td>-0.046052</td>\n",
       "      <td>0.024775</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.061016</td>\n",
       "      <td>-0.112826</td>\n",
       "      <td>-0.094258</td>\n",
       "      <td>-0.038224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pici</th>\n",
       "      <td>0.072495</td>\n",
       "      <td>0.190069</td>\n",
       "      <td>-0.010492</td>\n",
       "      <td>-0.079066</td>\n",
       "      <td>0.167971</td>\n",
       "      <td>-0.073624</td>\n",
       "      <td>-0.015284</td>\n",
       "      <td>0.180993</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.011248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092359</td>\n",
       "      <td>-0.071066</td>\n",
       "      <td>-0.010251</td>\n",
       "      <td>-0.067428</td>\n",
       "      <td>0.047717</td>\n",
       "      <td>0.141026</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>-0.121910</td>\n",
       "      <td>-0.049949</td>\n",
       "      <td>-0.056658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cotija_cheese</th>\n",
       "      <td>-0.000951</td>\n",
       "      <td>0.117041</td>\n",
       "      <td>0.115605</td>\n",
       "      <td>0.149431</td>\n",
       "      <td>-0.004248</td>\n",
       "      <td>0.031903</td>\n",
       "      <td>-0.053190</td>\n",
       "      <td>-0.116985</td>\n",
       "      <td>-0.069570</td>\n",
       "      <td>0.102118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-0.089873</td>\n",
       "      <td>-0.055630</td>\n",
       "      <td>-0.068586</td>\n",
       "      <td>0.121438</td>\n",
       "      <td>0.077678</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>0.105722</td>\n",
       "      <td>0.037894</td>\n",
       "      <td>-0.085962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nola</th>\n",
       "      <td>0.031371</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>-0.007650</td>\n",
       "      <td>0.174669</td>\n",
       "      <td>-0.017969</td>\n",
       "      <td>0.049377</td>\n",
       "      <td>-0.050669</td>\n",
       "      <td>-0.045430</td>\n",
       "      <td>-0.008090</td>\n",
       "      <td>-0.223163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108512</td>\n",
       "      <td>-0.123692</td>\n",
       "      <td>-0.093428</td>\n",
       "      <td>-0.040506</td>\n",
       "      <td>-0.083753</td>\n",
       "      <td>0.102802</td>\n",
       "      <td>-0.110456</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>-0.069582</td>\n",
       "      <td>0.093124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detract</th>\n",
       "      <td>-0.056579</td>\n",
       "      <td>-0.153620</td>\n",
       "      <td>0.275764</td>\n",
       "      <td>0.227959</td>\n",
       "      <td>0.138432</td>\n",
       "      <td>0.034924</td>\n",
       "      <td>-0.002331</td>\n",
       "      <td>-0.045096</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>0.059821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084003</td>\n",
       "      <td>0.035833</td>\n",
       "      <td>0.067942</td>\n",
       "      <td>0.051787</td>\n",
       "      <td>-0.192170</td>\n",
       "      <td>-0.072624</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.027619</td>\n",
       "      <td>-0.120581</td>\n",
       "      <td>-0.077085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^</th>\n",
       "      <td>-0.159364</td>\n",
       "      <td>-0.168161</td>\n",
       "      <td>-0.087906</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>-0.106490</td>\n",
       "      <td>0.139015</td>\n",
       "      <td>0.076675</td>\n",
       "      <td>0.136886</td>\n",
       "      <td>-0.183260</td>\n",
       "      <td>0.075733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>-0.025585</td>\n",
       "      <td>0.099331</td>\n",
       "      <td>-0.108664</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>-0.068774</td>\n",
       "      <td>0.189604</td>\n",
       "      <td>-0.155916</td>\n",
       "      <td>-0.114461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warrior</th>\n",
       "      <td>-0.094806</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.106258</td>\n",
       "      <td>0.142656</td>\n",
       "      <td>-0.176982</td>\n",
       "      <td>-0.036915</td>\n",
       "      <td>-0.031470</td>\n",
       "      <td>-0.134165</td>\n",
       "      <td>-0.019087</td>\n",
       "      <td>-0.074914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168627</td>\n",
       "      <td>0.127048</td>\n",
       "      <td>0.072818</td>\n",
       "      <td>-0.059963</td>\n",
       "      <td>0.015371</td>\n",
       "      <td>-0.005953</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>-0.081296</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>-0.047429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jambon</th>\n",
       "      <td>0.060012</td>\n",
       "      <td>0.225416</td>\n",
       "      <td>-0.002198</td>\n",
       "      <td>0.163080</td>\n",
       "      <td>0.034831</td>\n",
       "      <td>-0.080972</td>\n",
       "      <td>-0.059931</td>\n",
       "      <td>0.100770</td>\n",
       "      <td>-0.065641</td>\n",
       "      <td>0.046423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021358</td>\n",
       "      <td>-0.085486</td>\n",
       "      <td>-0.050476</td>\n",
       "      <td>0.047828</td>\n",
       "      <td>0.078148</td>\n",
       "      <td>0.133884</td>\n",
       "      <td>0.061358</td>\n",
       "      <td>0.121853</td>\n",
       "      <td>0.045201</td>\n",
       "      <td>-0.059471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cherry_coke</th>\n",
       "      <td>0.092691</td>\n",
       "      <td>0.161097</td>\n",
       "      <td>-0.019353</td>\n",
       "      <td>0.023907</td>\n",
       "      <td>-0.107412</td>\n",
       "      <td>0.082959</td>\n",
       "      <td>-0.018303</td>\n",
       "      <td>0.061914</td>\n",
       "      <td>-0.060058</td>\n",
       "      <td>-0.193048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078821</td>\n",
       "      <td>0.113618</td>\n",
       "      <td>-0.054142</td>\n",
       "      <td>-0.019945</td>\n",
       "      <td>0.262938</td>\n",
       "      <td>0.065415</td>\n",
       "      <td>0.078051</td>\n",
       "      <td>-0.056792</td>\n",
       "      <td>0.036505</td>\n",
       "      <td>-0.050319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there</th>\n",
       "      <td>0.091320</td>\n",
       "      <td>0.175059</td>\n",
       "      <td>0.174692</td>\n",
       "      <td>0.129620</td>\n",
       "      <td>0.021032</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.023298</td>\n",
       "      <td>0.050253</td>\n",
       "      <td>-0.026945</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.192759</td>\n",
       "      <td>0.023410</td>\n",
       "      <td>0.050899</td>\n",
       "      <td>0.034048</td>\n",
       "      <td>-0.064986</td>\n",
       "      <td>0.129877</td>\n",
       "      <td>0.007270</td>\n",
       "      <td>-0.126081</td>\n",
       "      <td>-0.016449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>0.072950</td>\n",
       "      <td>0.115621</td>\n",
       "      <td>0.171133</td>\n",
       "      <td>-0.125289</td>\n",
       "      <td>-0.050225</td>\n",
       "      <td>0.060336</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.191324</td>\n",
       "      <td>-0.127342</td>\n",
       "      <td>0.010864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.035516</td>\n",
       "      <td>-0.065138</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.042587</td>\n",
       "      <td>0.050504</td>\n",
       "      <td>-0.103641</td>\n",
       "      <td>0.121070</td>\n",
       "      <td>-0.079323</td>\n",
       "      <td>0.084219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>-0.142093</td>\n",
       "      <td>0.063393</td>\n",
       "      <td>-0.024548</td>\n",
       "      <td>-0.072159</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>-0.017314</td>\n",
       "      <td>0.075446</td>\n",
       "      <td>0.096343</td>\n",
       "      <td>-0.037456</td>\n",
       "      <td>0.041226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134934</td>\n",
       "      <td>0.062875</td>\n",
       "      <td>-0.084667</td>\n",
       "      <td>0.061148</td>\n",
       "      <td>-0.073450</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.065556</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>-0.019610</td>\n",
       "      <td>0.090010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-0.032502</td>\n",
       "      <td>0.128068</td>\n",
       "      <td>0.033786</td>\n",
       "      <td>-0.105357</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.095646</td>\n",
       "      <td>0.029846</td>\n",
       "      <td>0.057783</td>\n",
       "      <td>-0.013554</td>\n",
       "      <td>0.147964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085789</td>\n",
       "      <td>0.121038</td>\n",
       "      <td>0.171951</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.111467</td>\n",
       "      <td>0.126345</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>-0.118022</td>\n",
       "      <td>0.118856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.049869</td>\n",
       "      <td>0.217394</td>\n",
       "      <td>0.023123</td>\n",
       "      <td>0.009629</td>\n",
       "      <td>0.120602</td>\n",
       "      <td>0.073764</td>\n",
       "      <td>0.073446</td>\n",
       "      <td>-0.244737</td>\n",
       "      <td>-0.184412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096368</td>\n",
       "      <td>0.161749</td>\n",
       "      <td>-0.202004</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>0.009742</td>\n",
       "      <td>0.226267</td>\n",
       "      <td>-0.161991</td>\n",
       "      <td>0.061188</td>\n",
       "      <td>0.010244</td>\n",
       "      <td>-0.012053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.097627</td>\n",
       "      <td>-0.049518</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.142948</td>\n",
       "      <td>0.050310</td>\n",
       "      <td>0.052868</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>0.056520</td>\n",
       "      <td>-0.083100</td>\n",
       "      <td>0.044196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010982</td>\n",
       "      <td>0.107931</td>\n",
       "      <td>-0.151013</td>\n",
       "      <td>-0.038385</td>\n",
       "      <td>-0.158081</td>\n",
       "      <td>0.142615</td>\n",
       "      <td>0.077005</td>\n",
       "      <td>0.112522</td>\n",
       "      <td>-0.124649</td>\n",
       "      <td>-0.148734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>-0.032060</td>\n",
       "      <td>-0.044210</td>\n",
       "      <td>0.034033</td>\n",
       "      <td>0.064636</td>\n",
       "      <td>-0.042790</td>\n",
       "      <td>0.126739</td>\n",
       "      <td>0.130237</td>\n",
       "      <td>0.145650</td>\n",
       "      <td>-0.070552</td>\n",
       "      <td>-0.100472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123116</td>\n",
       "      <td>-0.050538</td>\n",
       "      <td>-0.028771</td>\n",
       "      <td>-0.006789</td>\n",
       "      <td>0.166310</td>\n",
       "      <td>0.099008</td>\n",
       "      <td>0.199809</td>\n",
       "      <td>-0.079326</td>\n",
       "      <td>-0.084235</td>\n",
       "      <td>0.132414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>0.124665</td>\n",
       "      <td>-0.011943</td>\n",
       "      <td>0.023524</td>\n",
       "      <td>-0.108308</td>\n",
       "      <td>0.062365</td>\n",
       "      <td>0.100183</td>\n",
       "      <td>0.012688</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>-0.112615</td>\n",
       "      <td>-0.094577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052842</td>\n",
       "      <td>-0.055043</td>\n",
       "      <td>-0.157417</td>\n",
       "      <td>0.096311</td>\n",
       "      <td>0.051305</td>\n",
       "      <td>0.096507</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>-0.028873</td>\n",
       "      <td>-0.054220</td>\n",
       "      <td>0.166465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.028910</td>\n",
       "      <td>0.048389</td>\n",
       "      <td>0.124926</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.062523</td>\n",
       "      <td>0.243127</td>\n",
       "      <td>0.104076</td>\n",
       "      <td>0.143462</td>\n",
       "      <td>-0.283467</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>-0.083843</td>\n",
       "      <td>0.011139</td>\n",
       "      <td>-0.021923</td>\n",
       "      <td>0.103380</td>\n",
       "      <td>0.060246</td>\n",
       "      <td>-0.115857</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.026253</td>\n",
       "      <td>0.081296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order</th>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.146241</td>\n",
       "      <td>0.080768</td>\n",
       "      <td>0.019245</td>\n",
       "      <td>0.110195</td>\n",
       "      <td>-0.024740</td>\n",
       "      <td>-0.055395</td>\n",
       "      <td>-0.017867</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.137016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060616</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.033293</td>\n",
       "      <td>-0.010258</td>\n",
       "      <td>0.081892</td>\n",
       "      <td>0.115267</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>-0.177321</td>\n",
       "      <td>-0.216230</td>\n",
       "      <td>0.085355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>-0.003722</td>\n",
       "      <td>0.052938</td>\n",
       "      <td>0.039691</td>\n",
       "      <td>0.084476</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>-0.169034</td>\n",
       "      <td>0.050739</td>\n",
       "      <td>0.131012</td>\n",
       "      <td>-0.033958</td>\n",
       "      <td>-0.059448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099616</td>\n",
       "      <td>0.072845</td>\n",
       "      <td>-0.053604</td>\n",
       "      <td>-0.098698</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.040971</td>\n",
       "      <td>-0.107248</td>\n",
       "      <td>-0.036277</td>\n",
       "      <td>0.045841</td>\n",
       "      <td>-0.103009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.079429</td>\n",
       "      <td>0.099208</td>\n",
       "      <td>0.052279</td>\n",
       "      <td>0.142055</td>\n",
       "      <td>0.112095</td>\n",
       "      <td>0.169546</td>\n",
       "      <td>-0.048188</td>\n",
       "      <td>0.059146</td>\n",
       "      <td>-0.153603</td>\n",
       "      <td>-0.057854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142731</td>\n",
       "      <td>0.145672</td>\n",
       "      <td>0.023334</td>\n",
       "      <td>-0.039482</td>\n",
       "      <td>-0.015132</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>-0.090902</td>\n",
       "      <td>0.074474</td>\n",
       "      <td>-0.170977</td>\n",
       "      <td>-0.019280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>-0.010891</td>\n",
       "      <td>0.085433</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>0.055013</td>\n",
       "      <td>0.190348</td>\n",
       "      <td>0.137881</td>\n",
       "      <td>0.112619</td>\n",
       "      <td>-0.141903</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.207722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041807</td>\n",
       "      <td>0.128888</td>\n",
       "      <td>0.037690</td>\n",
       "      <td>0.055328</td>\n",
       "      <td>0.013219</td>\n",
       "      <td>-0.047227</td>\n",
       "      <td>-0.061768</td>\n",
       "      <td>0.019032</td>\n",
       "      <td>0.048271</td>\n",
       "      <td>-0.033748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>-0.064063</td>\n",
       "      <td>0.072676</td>\n",
       "      <td>0.087668</td>\n",
       "      <td>-0.042169</td>\n",
       "      <td>0.118509</td>\n",
       "      <td>0.051332</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>0.145918</td>\n",
       "      <td>-0.059898</td>\n",
       "      <td>0.091178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070750</td>\n",
       "      <td>0.236482</td>\n",
       "      <td>0.201527</td>\n",
       "      <td>0.038116</td>\n",
       "      <td>0.067399</td>\n",
       "      <td>-0.110694</td>\n",
       "      <td>0.074625</td>\n",
       "      <td>0.037297</td>\n",
       "      <td>-0.101334</td>\n",
       "      <td>0.061843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>0.083498</td>\n",
       "      <td>0.161150</td>\n",
       "      <td>-0.062273</td>\n",
       "      <td>-0.074900</td>\n",
       "      <td>0.036567</td>\n",
       "      <td>0.106699</td>\n",
       "      <td>0.070516</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>-0.097962</td>\n",
       "      <td>-0.033224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.213749</td>\n",
       "      <td>0.025291</td>\n",
       "      <td>-0.001225</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>0.027791</td>\n",
       "      <td>0.137502</td>\n",
       "      <td>0.128606</td>\n",
       "      <td>-0.093125</td>\n",
       "      <td>0.089417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>-0.147472</td>\n",
       "      <td>0.012951</td>\n",
       "      <td>0.113644</td>\n",
       "      <td>0.047472</td>\n",
       "      <td>-0.027854</td>\n",
       "      <td>-0.113391</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>-0.183454</td>\n",
       "      <td>0.012985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052097</td>\n",
       "      <td>-0.086836</td>\n",
       "      <td>-0.073217</td>\n",
       "      <td>0.153150</td>\n",
       "      <td>0.204939</td>\n",
       "      <td>0.044067</td>\n",
       "      <td>-0.115989</td>\n",
       "      <td>-0.165668</td>\n",
       "      <td>-0.166631</td>\n",
       "      <td>-0.066722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>-0.047051</td>\n",
       "      <td>-0.066637</td>\n",
       "      <td>0.101922</td>\n",
       "      <td>0.157831</td>\n",
       "      <td>0.210948</td>\n",
       "      <td>0.093697</td>\n",
       "      <td>0.014509</td>\n",
       "      <td>0.185475</td>\n",
       "      <td>-0.300499</td>\n",
       "      <td>0.092525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>0.136370</td>\n",
       "      <td>-0.043328</td>\n",
       "      <td>-0.064213</td>\n",
       "      <td>0.049681</td>\n",
       "      <td>-0.091463</td>\n",
       "      <td>-0.057941</td>\n",
       "      <td>-0.062755</td>\n",
       "      <td>-0.148871</td>\n",
       "      <td>0.124590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>-0.176375</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>0.088423</td>\n",
       "      <td>0.135936</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>-0.056282</td>\n",
       "      <td>0.049946</td>\n",
       "      <td>0.104196</td>\n",
       "      <td>-0.178955</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102630</td>\n",
       "      <td>-0.034270</td>\n",
       "      <td>-0.071203</td>\n",
       "      <td>0.056525</td>\n",
       "      <td>0.133708</td>\n",
       "      <td>-0.033165</td>\n",
       "      <td>-0.100428</td>\n",
       "      <td>-0.086459</td>\n",
       "      <td>-0.179870</td>\n",
       "      <td>-0.048864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.044295</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.049939</td>\n",
       "      <td>0.107635</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>0.028974</td>\n",
       "      <td>0.104073</td>\n",
       "      <td>0.020335</td>\n",
       "      <td>0.089790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146471</td>\n",
       "      <td>-0.047002</td>\n",
       "      <td>-0.117427</td>\n",
       "      <td>-0.126984</td>\n",
       "      <td>-0.013206</td>\n",
       "      <td>0.051216</td>\n",
       "      <td>-0.035074</td>\n",
       "      <td>-0.034731</td>\n",
       "      <td>-0.129051</td>\n",
       "      <td>-0.076414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.093038</td>\n",
       "      <td>-0.052083</td>\n",
       "      <td>-0.001928</td>\n",
       "      <td>-0.101208</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>-0.079624</td>\n",
       "      <td>0.084303</td>\n",
       "      <td>0.062528</td>\n",
       "      <td>-0.169486</td>\n",
       "      <td>-0.063893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038358</td>\n",
       "      <td>0.080439</td>\n",
       "      <td>0.025073</td>\n",
       "      <td>0.114678</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.077478</td>\n",
       "      <td>0.104229</td>\n",
       "      <td>-0.186500</td>\n",
       "      <td>0.143378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.032088</td>\n",
       "      <td>-0.044843</td>\n",
       "      <td>0.159641</td>\n",
       "      <td>0.136753</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>-0.031905</td>\n",
       "      <td>0.047127</td>\n",
       "      <td>-0.209530</td>\n",
       "      <td>0.034079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049030</td>\n",
       "      <td>0.010308</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>-0.060682</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>-0.048569</td>\n",
       "      <td>0.121508</td>\n",
       "      <td>-0.044493</td>\n",
       "      <td>0.031478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.041835</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.044795</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>0.180355</td>\n",
       "      <td>0.136279</td>\n",
       "      <td>0.125675</td>\n",
       "      <td>0.067519</td>\n",
       "      <td>0.085606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127803</td>\n",
       "      <td>0.034769</td>\n",
       "      <td>-0.045143</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.207316</td>\n",
       "      <td>-0.039574</td>\n",
       "      <td>0.099479</td>\n",
       "      <td>-0.024263</td>\n",
       "      <td>-0.078957</td>\n",
       "      <td>0.057841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pizza</th>\n",
       "      <td>0.049582</td>\n",
       "      <td>-0.037574</td>\n",
       "      <td>0.047254</td>\n",
       "      <td>-0.149018</td>\n",
       "      <td>0.113984</td>\n",
       "      <td>-0.110883</td>\n",
       "      <td>-0.085655</td>\n",
       "      <td>0.249146</td>\n",
       "      <td>-0.009581</td>\n",
       "      <td>-0.176849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068895</td>\n",
       "      <td>-0.014070</td>\n",
       "      <td>-0.131388</td>\n",
       "      <td>-0.011482</td>\n",
       "      <td>0.131054</td>\n",
       "      <td>0.176260</td>\n",
       "      <td>-0.118280</td>\n",
       "      <td>0.105786</td>\n",
       "      <td>-0.072797</td>\n",
       "      <td>-0.135223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.036605</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>-0.029445</td>\n",
       "      <td>-0.022663</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>-0.091086</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>-0.076015</td>\n",
       "      <td>-0.054357</td>\n",
       "      <td>-0.111706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087624</td>\n",
       "      <td>0.075543</td>\n",
       "      <td>-0.020226</td>\n",
       "      <td>0.048019</td>\n",
       "      <td>-0.022710</td>\n",
       "      <td>0.051343</td>\n",
       "      <td>-0.193222</td>\n",
       "      <td>-0.045574</td>\n",
       "      <td>-0.012759</td>\n",
       "      <td>-0.031463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>-0.011043</td>\n",
       "      <td>-0.006366</td>\n",
       "      <td>0.183424</td>\n",
       "      <td>-0.116111</td>\n",
       "      <td>0.127045</td>\n",
       "      <td>-0.080610</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.122632</td>\n",
       "      <td>-0.027150</td>\n",
       "      <td>-0.124283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015726</td>\n",
       "      <td>0.169981</td>\n",
       "      <td>-0.097249</td>\n",
       "      <td>0.027040</td>\n",
       "      <td>0.101849</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.121866</td>\n",
       "      <td>-0.094503</td>\n",
       "      <td>-0.089829</td>\n",
       "      <td>0.126752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.018479</td>\n",
       "      <td>0.026108</td>\n",
       "      <td>0.084202</td>\n",
       "      <td>-0.099403</td>\n",
       "      <td>-0.013599</td>\n",
       "      <td>0.093818</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>0.091414</td>\n",
       "      <td>-0.113425</td>\n",
       "      <td>0.046550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064292</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>-0.022587</td>\n",
       "      <td>0.041592</td>\n",
       "      <td>-0.079762</td>\n",
       "      <td>0.031079</td>\n",
       "      <td>0.038529</td>\n",
       "      <td>0.057772</td>\n",
       "      <td>-0.245938</td>\n",
       "      <td>0.198615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.091916</td>\n",
       "      <td>0.150838</td>\n",
       "      <td>0.092004</td>\n",
       "      <td>0.059429</td>\n",
       "      <td>0.099397</td>\n",
       "      <td>0.085207</td>\n",
       "      <td>0.146888</td>\n",
       "      <td>0.155916</td>\n",
       "      <td>-0.084354</td>\n",
       "      <td>-0.043961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037090</td>\n",
       "      <td>0.169076</td>\n",
       "      <td>0.054787</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>0.065545</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.030463</td>\n",
       "      <td>0.014737</td>\n",
       "      <td>-0.086194</td>\n",
       "      <td>0.153399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.036488</td>\n",
       "      <td>0.019838</td>\n",
       "      <td>0.094594</td>\n",
       "      <td>0.034265</td>\n",
       "      <td>0.022982</td>\n",
       "      <td>0.049892</td>\n",
       "      <td>0.217917</td>\n",
       "      <td>0.120926</td>\n",
       "      <td>0.117482</td>\n",
       "      <td>-0.020895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166388</td>\n",
       "      <td>0.134057</td>\n",
       "      <td>0.022242</td>\n",
       "      <td>0.195783</td>\n",
       "      <td>0.011698</td>\n",
       "      <td>0.058571</td>\n",
       "      <td>0.011964</td>\n",
       "      <td>0.073284</td>\n",
       "      <td>-0.176840</td>\n",
       "      <td>0.096062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>0.115631</td>\n",
       "      <td>-0.053218</td>\n",
       "      <td>0.079125</td>\n",
       "      <td>-0.022383</td>\n",
       "      <td>0.048496</td>\n",
       "      <td>0.082950</td>\n",
       "      <td>0.060959</td>\n",
       "      <td>0.098032</td>\n",
       "      <td>-0.154927</td>\n",
       "      <td>0.049133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061183</td>\n",
       "      <td>0.089757</td>\n",
       "      <td>0.013545</td>\n",
       "      <td>-0.014514</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>-0.049307</td>\n",
       "      <td>-0.093320</td>\n",
       "      <td>0.138060</td>\n",
       "      <td>-0.144756</td>\n",
       "      <td>0.103025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-PRON-</th>\n",
       "      <td>0.064767</td>\n",
       "      <td>0.095335</td>\n",
       "      <td>0.073501</td>\n",
       "      <td>0.081706</td>\n",
       "      <td>0.032422</td>\n",
       "      <td>-0.026943</td>\n",
       "      <td>0.090121</td>\n",
       "      <td>0.034908</td>\n",
       "      <td>-0.063289</td>\n",
       "      <td>-0.235578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055878</td>\n",
       "      <td>0.121511</td>\n",
       "      <td>-0.001210</td>\n",
       "      <td>-0.071555</td>\n",
       "      <td>0.092081</td>\n",
       "      <td>0.103808</td>\n",
       "      <td>-0.044214</td>\n",
       "      <td>-0.101631</td>\n",
       "      <td>-0.175123</td>\n",
       "      <td>0.005848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16752 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2         3         4   \\\n",
       "shipwreck_burger      -0.008992  0.069071 -0.044403 -0.066379 -0.110571   \n",
       "fat_uncle_tony        -0.110202  0.124813  0.024812 -0.116159  0.239851   \n",
       "dianoia               -0.076524  0.178293 -0.033832  0.115640  0.043627   \n",
       "lamb_chorizo          -0.115281  0.116881  0.059889  0.090417  0.036778   \n",
       "organ_music           -0.049580 -0.123448  0.139327  0.040766 -0.093590   \n",
       "secret_hallway        -0.011642  0.007484 -0.015406  0.147396 -0.061920   \n",
       "5am                    0.064431  0.013835 -0.101313  0.066900 -0.139770   \n",
       "apiecalypse           -0.189204  0.015161  0.094419 -0.101405 -0.085885   \n",
       "jicama_fry            -0.040205  0.169634  0.004891  0.082035 -0.132468   \n",
       "joanna                 0.042130  0.051496 -0.023502 -0.024183  0.220385   \n",
       "emeril                -0.024246  0.017684  0.112363  0.161033 -0.062193   \n",
       "yoli                  -0.020396  0.063914  0.085350  0.152904 -0.055640   \n",
       "brimstone             -0.134403 -0.007808 -0.040262 -0.006577  0.036879   \n",
       "soppressata_marmalade -0.050193 -0.086515 -0.008821  0.013409  0.137798   \n",
       "choc_chip             -0.073072  0.184384  0.005703 -0.006679 -0.178735   \n",
       "vero_bistro           -0.181221  0.074585  0.137863  0.047094  0.082426   \n",
       "oberlin               -0.007305 -0.080906  0.074522  0.085458 -0.005709   \n",
       "aria_hotel             0.015693 -0.047445 -0.054489  0.213119 -0.170043   \n",
       "shady_park            -0.219032 -0.094213 -0.012360  0.026172 -0.020752   \n",
       "caprese_bruschetta    -0.011774  0.130502  0.120560 -0.169919  0.091245   \n",
       "davinci               -0.038101  0.199305  0.014235 -0.183532  0.100110   \n",
       "hor                    0.133340  0.198932 -0.045366  0.032122 -0.034952   \n",
       "pici                   0.072495  0.190069 -0.010492 -0.079066  0.167971   \n",
       "cotija_cheese         -0.000951  0.117041  0.115605  0.149431 -0.004248   \n",
       "nola                   0.031371  0.003414 -0.007650  0.174669 -0.017969   \n",
       "detract               -0.056579 -0.153620  0.275764  0.227959  0.138432   \n",
       "^                     -0.159364 -0.168161 -0.087906  0.039400 -0.106490   \n",
       "warrior               -0.094806  0.002124  0.106258  0.142656 -0.176982   \n",
       "jambon                 0.060012  0.225416 -0.002198  0.163080  0.034831   \n",
       "cherry_coke            0.092691  0.161097 -0.019353  0.023907 -0.107412   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "there                  0.091320  0.175059  0.174692  0.129620  0.021032   \n",
       "can                    0.072950  0.115621  0.171133 -0.125289 -0.050225   \n",
       "like                  -0.142093  0.063393 -0.024548 -0.072159  0.006895   \n",
       "great                 -0.032502  0.128068  0.033786 -0.105357  0.007525   \n",
       "food                   0.056900  0.049869  0.217394  0.023123  0.009629   \n",
       "at                     0.097627 -0.049518  0.009173  0.142948  0.050310   \n",
       "go                    -0.032060 -0.044210  0.034033  0.064636 -0.042790   \n",
       "get                    0.124665 -0.011943  0.023524 -0.108308  0.062365   \n",
       "so                     0.028910  0.048389  0.124926  0.002530  0.062523   \n",
       "order                  0.026245  0.146241  0.080768  0.019245  0.110195   \n",
       "place                 -0.003722  0.052938  0.039691  0.084476  0.011198   \n",
       "do                     0.079429  0.099208  0.052279  0.142055  0.112095   \n",
       "on                    -0.010891  0.085433 -0.009099  0.055013  0.190348   \n",
       "good                  -0.064063  0.072676  0.087668 -0.042169  0.118509   \n",
       "with                   0.083498  0.161150 -0.062273 -0.074900  0.036567   \n",
       "this                  -0.147472  0.012951  0.113644  0.047472 -0.027854   \n",
       "but                   -0.047051 -0.066637  0.101922  0.157831  0.210948   \n",
       "that                  -0.176375  0.068455  0.088423  0.135936  0.019176   \n",
       "in                     0.044295  0.001777  0.049939  0.107635  0.053800   \n",
       "for                    0.093038 -0.052083 -0.001928 -0.101208  0.084486   \n",
       "not                    0.016650  0.032088 -0.044843  0.159641  0.136753   \n",
       "of                    -0.041835  0.004703  0.021322  0.044795  0.016956   \n",
       "pizza                  0.049582 -0.037574  0.047254 -0.149018  0.113984   \n",
       "to                     0.036605  0.118280 -0.029445 -0.022663  0.060991   \n",
       "have                  -0.011043 -0.006366  0.183424 -0.116111  0.127045   \n",
       "a                      0.018479  0.026108  0.084202 -0.099403 -0.013599   \n",
       "and                    0.091916  0.150838  0.092004  0.059429  0.099397   \n",
       "the                    0.036488  0.019838  0.094594  0.034265  0.022982   \n",
       "be                     0.115631 -0.053218  0.079125 -0.022383  0.048496   \n",
       "-PRON-                 0.064767  0.095335  0.073501  0.081706  0.032422   \n",
       "\n",
       "                             5         6         7         8         9   \\\n",
       "shipwreck_burger      -0.051169  0.096304  0.020024  0.033676 -0.148641   \n",
       "fat_uncle_tony         0.057880  0.018485  0.161749  0.024854  0.074692   \n",
       "dianoia                0.158894  0.154938  0.072801  0.071337 -0.077997   \n",
       "lamb_chorizo           0.034917 -0.024799  0.172585  0.021246  0.043193   \n",
       "organ_music            0.176509 -0.108355 -0.063220 -0.013871  0.020749   \n",
       "secret_hallway        -0.062190 -0.090432  0.142934 -0.017427 -0.092649   \n",
       "5am                    0.105381 -0.216017  0.086563 -0.062952 -0.033686   \n",
       "apiecalypse            0.050780 -0.029000  0.145084  0.056160  0.155907   \n",
       "jicama_fry             0.064700  0.086396  0.129385 -0.026789 -0.144426   \n",
       "joanna                 0.064851  0.178014  0.093679  0.058013 -0.028825   \n",
       "emeril                -0.009788 -0.028214 -0.112537  0.092924 -0.029265   \n",
       "yoli                  -0.226780  0.077708 -0.023281 -0.050898 -0.034766   \n",
       "brimstone              0.000967 -0.042600  0.004604  0.194187 -0.017311   \n",
       "soppressata_marmalade -0.040572  0.105973  0.090856  0.121990  0.039391   \n",
       "choc_chip              0.076444 -0.057219  0.083627  0.022767 -0.009538   \n",
       "vero_bistro            0.014876  0.060102  0.038484  0.026909 -0.043411   \n",
       "oberlin                0.020881 -0.109144  0.164599  0.102354  0.013921   \n",
       "aria_hotel             0.127753  0.049336  0.123832  0.019755  0.005595   \n",
       "shady_park             0.094453  0.128042  0.022693 -0.065575  0.096917   \n",
       "caprese_bruschetta     0.138024  0.125142  0.253348  0.025978  0.086949   \n",
       "davinci               -0.137999  0.191221  0.017408  0.165447  0.122837   \n",
       "hor                    0.165558  0.136696  0.003036  0.066247  0.107235   \n",
       "pici                  -0.073624 -0.015284  0.180993  0.026566  0.011248   \n",
       "cotija_cheese          0.031903 -0.053190 -0.116985 -0.069570  0.102118   \n",
       "nola                   0.049377 -0.050669 -0.045430 -0.008090 -0.223163   \n",
       "detract                0.034924 -0.002331 -0.045096 -0.064249  0.059821   \n",
       "^                      0.139015  0.076675  0.136886 -0.183260  0.075733   \n",
       "warrior               -0.036915 -0.031470 -0.134165 -0.019087 -0.074914   \n",
       "jambon                -0.080972 -0.059931  0.100770 -0.065641  0.046423   \n",
       "cherry_coke            0.082959 -0.018303  0.061914 -0.060058 -0.193048   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "there                  0.001677  0.023298  0.050253 -0.026945  0.006022   \n",
       "can                    0.060336  0.020561  0.191324 -0.127342  0.010864   \n",
       "like                  -0.017314  0.075446  0.096343 -0.037456  0.041226   \n",
       "great                  0.095646  0.029846  0.057783 -0.013554  0.147964   \n",
       "food                   0.120602  0.073764  0.073446 -0.244737 -0.184412   \n",
       "at                     0.052868 -0.039551  0.056520 -0.083100  0.044196   \n",
       "go                     0.126739  0.130237  0.145650 -0.070552 -0.100472   \n",
       "get                    0.100183  0.012688  0.054638 -0.112615 -0.094577   \n",
       "so                     0.243127  0.104076  0.143462 -0.283467  0.006183   \n",
       "order                 -0.024740 -0.055395 -0.017867 -0.004741 -0.137016   \n",
       "place                 -0.169034  0.050739  0.131012 -0.033958 -0.059448   \n",
       "do                     0.169546 -0.048188  0.059146 -0.153603 -0.057854   \n",
       "on                     0.137881  0.112619 -0.141903  0.022991  0.207722   \n",
       "good                   0.051332  0.009533  0.145918 -0.059898  0.091178   \n",
       "with                   0.106699  0.070516  0.086147 -0.097962 -0.033224   \n",
       "this                  -0.113391  0.017699  0.012222 -0.183454  0.012985   \n",
       "but                    0.093697  0.014509  0.185475 -0.300499  0.092525   \n",
       "that                  -0.056282  0.049946  0.104196 -0.178955 -0.000354   \n",
       "in                    -0.002116  0.028974  0.104073  0.020335  0.089790   \n",
       "for                   -0.079624  0.084303  0.062528 -0.169486 -0.063893   \n",
       "not                    0.002773 -0.031905  0.047127 -0.209530  0.034079   \n",
       "of                     0.180355  0.136279  0.125675  0.067519  0.085606   \n",
       "pizza                 -0.110883 -0.085655  0.249146 -0.009581 -0.176849   \n",
       "to                    -0.091086  0.050952 -0.076015 -0.054357 -0.111706   \n",
       "have                  -0.080610  0.015040  0.122632 -0.027150 -0.124283   \n",
       "a                      0.093818  0.046382  0.091414 -0.113425  0.046550   \n",
       "and                    0.085207  0.146888  0.155916 -0.084354 -0.043961   \n",
       "the                    0.049892  0.217917  0.120926  0.117482 -0.020895   \n",
       "be                     0.082950  0.060959  0.098032 -0.154927  0.049133   \n",
       "-PRON-                -0.026943  0.090121  0.034908 -0.063289 -0.235578   \n",
       "\n",
       "                         ...           90        91        92        93  \\\n",
       "shipwreck_burger         ...    -0.116319 -0.141442 -0.031097 -0.092572   \n",
       "fat_uncle_tony           ...     0.045939 -0.074423 -0.024706 -0.006327   \n",
       "dianoia                  ...    -0.052969 -0.049967  0.075205 -0.054349   \n",
       "lamb_chorizo             ...    -0.080942  0.068433  0.016908  0.045230   \n",
       "organ_music              ...     0.014698  0.006284 -0.097220 -0.056107   \n",
       "secret_hallway           ...    -0.056522 -0.127429 -0.080508  0.077457   \n",
       "5am                      ...    -0.128176 -0.069481  0.009166  0.165028   \n",
       "apiecalypse              ...     0.077971 -0.122632 -0.172046  0.116918   \n",
       "jicama_fry               ...    -0.031000  0.045968  0.007623 -0.015678   \n",
       "joanna                   ...    -0.057799 -0.059719  0.015825 -0.007224   \n",
       "emeril                   ...    -0.094892 -0.101644 -0.060094 -0.104245   \n",
       "yoli                     ...     0.019899  0.064196  0.087739 -0.241405   \n",
       "brimstone                ...    -0.134744 -0.023471  0.011465 -0.154001   \n",
       "soppressata_marmalade    ...     0.049613 -0.041060 -0.047666  0.039599   \n",
       "choc_chip                ...     0.017959 -0.076338  0.083766 -0.082891   \n",
       "vero_bistro              ...     0.052806 -0.130720 -0.067285 -0.192226   \n",
       "oberlin                  ...    -0.038243  0.075031 -0.161663 -0.131113   \n",
       "aria_hotel               ...    -0.145236  0.023013 -0.119692  0.038456   \n",
       "shady_park               ...     0.046334 -0.016325 -0.133936 -0.037850   \n",
       "caprese_bruschetta       ...    -0.135232 -0.079342  0.093287  0.043018   \n",
       "davinci                  ...    -0.129809 -0.035600 -0.001983  0.072397   \n",
       "hor                      ...     0.018533  0.055166 -0.067630 -0.046052   \n",
       "pici                     ...    -0.092359 -0.071066 -0.010251 -0.067428   \n",
       "cotija_cheese            ...     0.001080 -0.089873 -0.055630 -0.068586   \n",
       "nola                     ...     0.108512 -0.123692 -0.093428 -0.040506   \n",
       "detract                  ...     0.084003  0.035833  0.067942  0.051787   \n",
       "^                        ...     0.017782  0.032990 -0.025585  0.099331   \n",
       "warrior                  ...    -0.168627  0.127048  0.072818 -0.059963   \n",
       "jambon                   ...    -0.021358 -0.085486 -0.050476  0.047828   \n",
       "cherry_coke              ...    -0.078821  0.113618 -0.054142 -0.019945   \n",
       "...                      ...          ...       ...       ...       ...   \n",
       "there                    ...     0.201497  0.192759  0.023410  0.050899   \n",
       "can                      ...     0.003163  0.035516 -0.065138  0.004830   \n",
       "like                     ...     0.134934  0.062875 -0.084667  0.061148   \n",
       "great                    ...     0.085789  0.121038  0.171951  0.015128   \n",
       "food                     ...    -0.096368  0.161749 -0.202004 -0.014709   \n",
       "at                       ...    -0.010982  0.107931 -0.151013 -0.038385   \n",
       "go                       ...    -0.123116 -0.050538 -0.028771 -0.006789   \n",
       "get                      ...    -0.052842 -0.055043 -0.157417  0.096311   \n",
       "so                       ...     0.010260 -0.083843  0.011139 -0.021923   \n",
       "order                    ...    -0.060616  0.007726  0.033293 -0.010258   \n",
       "place                    ...     0.099616  0.072845 -0.053604 -0.098698   \n",
       "do                       ...    -0.142731  0.145672  0.023334 -0.039482   \n",
       "on                       ...     0.041807  0.128888  0.037690  0.055328   \n",
       "good                     ...     0.070750  0.236482  0.201527  0.038116   \n",
       "with                     ...     0.169492  0.213749  0.025291 -0.001225   \n",
       "this                     ...     0.052097 -0.086836 -0.073217  0.153150   \n",
       "but                      ...    -0.046228  0.136370 -0.043328 -0.064213   \n",
       "that                     ...     0.102630 -0.034270 -0.071203  0.056525   \n",
       "in                       ...     0.146471 -0.047002 -0.117427 -0.126984   \n",
       "for                      ...     0.038358  0.080439  0.025073  0.114678   \n",
       "not                      ...     0.049030  0.010308  0.016908 -0.060682   \n",
       "of                       ...     0.127803  0.034769 -0.045143  0.018286   \n",
       "pizza                    ...    -0.068895 -0.014070 -0.131388 -0.011482   \n",
       "to                       ...     0.087624  0.075543 -0.020226  0.048019   \n",
       "have                     ...     0.015726  0.169981 -0.097249  0.027040   \n",
       "a                        ...     0.064292  0.096154 -0.022587  0.041592   \n",
       "and                      ...     0.037090  0.169076  0.054787 -0.018311   \n",
       "the                      ...    -0.166388  0.134057  0.022242  0.195783   \n",
       "be                       ...    -0.061183  0.089757  0.013545 -0.014514   \n",
       "-PRON-                   ...     0.055878  0.121511 -0.001210 -0.071555   \n",
       "\n",
       "                             94        95        96        97        98  \\\n",
       "shipwreck_burger       0.182790  0.024603  0.068332 -0.093378  0.032352   \n",
       "fat_uncle_tony         0.113073  0.184647  0.044923 -0.068010  0.037040   \n",
       "dianoia                0.098447  0.025781 -0.023025 -0.035267  0.073016   \n",
       "lamb_chorizo           0.045128  0.104081  0.159248 -0.066578  0.007341   \n",
       "organ_music           -0.158724 -0.071799 -0.026583  0.028074 -0.068363   \n",
       "secret_hallway         0.084034 -0.116520 -0.086455  0.104335  0.011970   \n",
       "5am                    0.023574 -0.016147 -0.039076  0.138584  0.040436   \n",
       "apiecalypse            0.071860  0.039033 -0.064204  0.038068  0.036973   \n",
       "jicama_fry             0.237579 -0.151209  0.134013  0.115267  0.083593   \n",
       "joanna                -0.094543 -0.010705  0.086375 -0.033077 -0.097246   \n",
       "emeril                -0.165910  0.096416  0.009050 -0.023789  0.059632   \n",
       "yoli                   0.078495  0.050972 -0.004864 -0.168966  0.044509   \n",
       "brimstone              0.062192 -0.058664  0.113100  0.036117  0.124316   \n",
       "soppressata_marmalade  0.136480  0.116905  0.105467  0.002885  0.016133   \n",
       "choc_chip              0.121686 -0.084918  0.026835  0.121846 -0.075995   \n",
       "vero_bistro           -0.074968  0.046571 -0.123345 -0.035305  0.009148   \n",
       "oberlin               -0.036827 -0.006584 -0.045671 -0.135558 -0.085684   \n",
       "aria_hotel             0.005336 -0.091374 -0.212037  0.067723  0.044053   \n",
       "shady_park            -0.050931 -0.033585 -0.102373  0.001027 -0.023786   \n",
       "caprese_bruschetta     0.260069  0.040450 -0.064958 -0.107730 -0.022220   \n",
       "davinci               -0.013816 -0.004943  0.061289 -0.148010  0.101043   \n",
       "hor                    0.024775  0.091102  0.061016 -0.112826 -0.094258   \n",
       "pici                   0.047717  0.141026  0.073254 -0.121910 -0.049949   \n",
       "cotija_cheese          0.121438  0.077678 -0.000609  0.105722  0.037894   \n",
       "nola                  -0.083753  0.102802 -0.110456  0.004980 -0.069582   \n",
       "detract               -0.192170 -0.072624  0.002248  0.027619 -0.120581   \n",
       "^                     -0.108664  0.003813 -0.068774  0.189604 -0.155916   \n",
       "warrior                0.015371 -0.005953  0.126900 -0.081296  0.094252   \n",
       "jambon                 0.078148  0.133884  0.061358  0.121853  0.045201   \n",
       "cherry_coke            0.262938  0.065415  0.078051 -0.056792  0.036505   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "there                  0.034048 -0.064986  0.129877  0.007270 -0.126081   \n",
       "can                    0.042587  0.050504 -0.103641  0.121070 -0.079323   \n",
       "like                  -0.073450  0.017971  0.065556  0.264300 -0.019610   \n",
       "great                  0.000747 -0.111467  0.126345  0.005687 -0.118022   \n",
       "food                   0.009742  0.226267 -0.161991  0.061188  0.010244   \n",
       "at                    -0.158081  0.142615  0.077005  0.112522 -0.124649   \n",
       "go                     0.166310  0.099008  0.199809 -0.079326 -0.084235   \n",
       "get                    0.051305  0.096507  0.169900 -0.028873 -0.054220   \n",
       "so                     0.103380  0.060246 -0.115857  0.024265  0.026253   \n",
       "order                  0.081892  0.115267  0.021885 -0.177321 -0.216230   \n",
       "place                  0.020274  0.040971 -0.107248 -0.036277  0.045841   \n",
       "do                    -0.015132  0.058042 -0.090902  0.074474 -0.170977   \n",
       "on                     0.013219 -0.047227 -0.061768  0.019032  0.048271   \n",
       "good                   0.067399 -0.110694  0.074625  0.037297 -0.101334   \n",
       "with                   0.021732  0.027791  0.137502  0.128606 -0.093125   \n",
       "this                   0.204939  0.044067 -0.115989 -0.165668 -0.166631   \n",
       "but                    0.049681 -0.091463 -0.057941 -0.062755 -0.148871   \n",
       "that                   0.133708 -0.033165 -0.100428 -0.086459 -0.179870   \n",
       "in                    -0.013206  0.051216 -0.035074 -0.034731 -0.129051   \n",
       "for                    0.021375  0.021183  0.077478  0.104229 -0.186500   \n",
       "not                    0.075758  0.061290 -0.048569  0.121508 -0.044493   \n",
       "of                     0.207316 -0.039574  0.099479 -0.024263 -0.078957   \n",
       "pizza                  0.131054  0.176260 -0.118280  0.105786 -0.072797   \n",
       "to                    -0.022710  0.051343 -0.193222 -0.045574 -0.012759   \n",
       "have                   0.101849  0.012953  0.121866 -0.094503 -0.089829   \n",
       "a                     -0.079762  0.031079  0.038529  0.057772 -0.245938   \n",
       "and                    0.065545  0.001197  0.030463  0.014737 -0.086194   \n",
       "the                    0.011698  0.058571  0.011964  0.073284 -0.176840   \n",
       "be                    -0.009482 -0.049307 -0.093320  0.138060 -0.144756   \n",
       "-PRON-                 0.092081  0.103808 -0.044214 -0.101631 -0.175123   \n",
       "\n",
       "                             99  \n",
       "shipwreck_burger       0.063388  \n",
       "fat_uncle_tony        -0.089825  \n",
       "dianoia               -0.060536  \n",
       "lamb_chorizo           0.064715  \n",
       "organ_music           -0.097564  \n",
       "secret_hallway         0.142961  \n",
       "5am                   -0.069485  \n",
       "apiecalypse           -0.160624  \n",
       "jicama_fry            -0.109475  \n",
       "joanna                 0.027874  \n",
       "emeril                 0.066779  \n",
       "yoli                  -0.019740  \n",
       "brimstone             -0.001063  \n",
       "soppressata_marmalade  0.002577  \n",
       "choc_chip             -0.016449  \n",
       "vero_bistro           -0.130798  \n",
       "oberlin                0.090239  \n",
       "aria_hotel             0.114976  \n",
       "shady_park            -0.087078  \n",
       "caprese_bruschetta     0.026259  \n",
       "davinci               -0.200894  \n",
       "hor                   -0.038224  \n",
       "pici                  -0.056658  \n",
       "cotija_cheese         -0.085962  \n",
       "nola                   0.093124  \n",
       "detract               -0.077085  \n",
       "^                     -0.114461  \n",
       "warrior               -0.047429  \n",
       "jambon                -0.059471  \n",
       "cherry_coke           -0.050319  \n",
       "...                         ...  \n",
       "there                 -0.016449  \n",
       "can                    0.084219  \n",
       "like                   0.090010  \n",
       "great                  0.118856  \n",
       "food                  -0.012053  \n",
       "at                    -0.148734  \n",
       "go                     0.132414  \n",
       "get                    0.166465  \n",
       "so                     0.081296  \n",
       "order                  0.085355  \n",
       "place                 -0.103009  \n",
       "do                    -0.019280  \n",
       "on                    -0.033748  \n",
       "good                   0.061843  \n",
       "with                   0.089417  \n",
       "this                  -0.066722  \n",
       "but                    0.124590  \n",
       "that                  -0.048864  \n",
       "in                    -0.076414  \n",
       "for                    0.143378  \n",
       "not                    0.031478  \n",
       "of                     0.057841  \n",
       "pizza                 -0.135223  \n",
       "to                    -0.031463  \n",
       "have                   0.126752  \n",
       "a                      0.198615  \n",
       "and                    0.153399  \n",
       "the                    0.096062  \n",
       "be                     0.103025  \n",
       "-PRON-                 0.005848  \n",
       "\n",
       "[16752 rows x 100 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in food2vec.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda term_index_count: -term_index_count[1])\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(food2vec.wv.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a t-sne reduction of the data into x-y coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "# create a stop word object.\n",
    "tsne_input = word_vectors.drop(STOPWORDS, errors=u'ignore')\n",
    "# create an input of 5000.\n",
    "tsne_input = tsne_input.head(5000)\n",
    "# create a tsne model path.\n",
    "tsne_filepath = os.path.join(intermediate_directory,\n",
    "                             u'tsne_model')\n",
    "# create a tsne vectors path object.\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory,\n",
    "                                     u'tsne_vectors.npy')\n",
    "\n",
    "\n",
    "%%time\n",
    "\n",
    "if 0 == 1:\n",
    "    # Instantiate a TSNE model\n",
    "    tsne = TSNE()\n",
    "    # fit the TSNE model\n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values)\n",
    "    # Save to pickle file\n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne, f)\n",
    "    # Save the tsne vectors\n",
    "    pd.np.save(tsne_vectors_filepath, tsne_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a pandas data frame version of the tsne word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(tsne_filepath) as f:\n",
    "#    tsne = pickle.load(f)\n",
    "    \n",
    "tsne_vectors = pd.np.load(tsne_vectors_filepath)\n",
    "\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(tsne_input.index),\n",
    "                            columns=[u'x_coord', u'y_coord'])\n",
    "tsne_vectors[u'word'] = tsne_vectors.index\n",
    "tsneMatrix = tsne_vectors[['x_coord', 'y_coord']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a variety of k means objects to find the optimal Silhouette Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the needed packages\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "# create an empty lists for the sum of squares\n",
    "i = []\n",
    "# cast the tsne vectors to a matrix\n",
    "m = np.matrix(tsne_vectors[['x_coord', 'y_coord']])\n",
    "# create a list of the kernal values\n",
    "kernals = np.array(range(2, 25))\n",
    "\n",
    "Sscores = []\n",
    "\n",
    "for k in kernals:\n",
    "    # create a Kmeans with k number of kernals\n",
    "    km = KMeans(n_clusters=k, init='random')\n",
    "    # append sum of squares\n",
    "    i.append(km.fit(m).inertia_)\n",
    "    # fitcluster labels\n",
    "    clusterLabels = km.fit_predict(m)\n",
    "    # append silhouette scores\n",
    "    Sscores.append(silhouette_score(m, clusterLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kdf = pd.DataFrame(np.array(Sscores), np.array(kernals)).reset_index()\n",
    "Kdf.columns = ['Kernals', 'Silhouette Score', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a274bcfd0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8hPB78RWVavyQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3ba45908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "sns.pointplot(x='Kernals', y = 'Silhouette Score', data = Kdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the optimal number of kernals is 3, though perhaps 7 will work better if it appears that 3 is not enough. Below we create models for 3 and 7 kernals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KMeans model with 3 clusters \n",
    "km3 = KMeans(n_clusters = 3, init = 'random')\n",
    "# KMeans model with 7 clusters\n",
    "km7 = KMeans(n_clusters = 7, init = 'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next tran DBscan and the 2 Kmeans labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the sklearn DBscan and SpectralClustering libraries\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "cluster = tsneMatrix\n",
    "\n",
    "cluster['cluster3kmean'] = km3.fit_predict(tsneMatrix)\n",
    "cluster['cluster7kmean'] = km7.fit_predict(tsneMatrix)\n",
    "cluster['clusterdbscan'] = DBSCAN(eps=1.0).fit_predict(tsneMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we create what was ultimately used, a spectral clustering model with neareast neighbors affinity and kmeans labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a copy of the tsneMatrix\n",
    "ClusteredM = tsneMatrix\n",
    "\n",
    "# create the Spectral Clustering object\n",
    "sc = SpectralClustering(affinity = 'nearest_neighbors', assign_labels = 'kmeans')\n",
    "\n",
    "# fit the cluster labels \n",
    "ClusteredM['cluster'] = sc.fit_predict(tsneMatrix)\n",
    "\n",
    "# print the unique cluster counts.\n",
    "np.unique(ClusteredM.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format the clustered data frame\n",
    "ClusteredM.head()\n",
    "t = ClusteredM.reset_index()\n",
    "t.columns = ['word', 'x_coord', 'y_coord', 'cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Next we will plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"a56fa995-48bf-41d9-957e-1b5b6ee52df1\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"a56fa995-48bf-41d9-957e-1b5b6ee52df1\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"a56fa995-48bf-41d9-957e-1b5b6ee52df1\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'a56fa995-48bf-41d9-957e-1b5b6ee52df1' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"a56fa995-48bf-41d9-957e-1b5b6ee52df1\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"a56fa995-48bf-41d9-957e-1b5b6ee52df1\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"a56fa995-48bf-41d9-957e-1b5b6ee52df1\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'a56fa995-48bf-41d9-957e-1b5b6ee52df1' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"a56fa995-48bf-41d9-957e-1b5b6ee52df1\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add our DataFrame as a ColumnDataSource for Bokeh\n",
    "#plot_data = ColumnDataSource(tsneMatrix)\n",
    "from bokeh.resources import CDN\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.io import output_file, show\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tsne_plot = figure(title=u't-SNE Word Embeddings',\n",
    "                   output_backend=\"webgl\",\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800,\n",
    "                   tools= (u'pan, wheel_zoom, box_zoom,'\n",
    "                           u'box_select, reset'), #, resize\n",
    "                   active_scroll=u'wheel_zoom')\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools( HoverTool(tooltips = u'@words') )\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 0]),\n",
    "                 color=u'Purple', line_alpha=0.2, fill_alpha=0.1, \n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 1]),#legend = u'cluster',\n",
    "                 color=u'navy', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 2]),#legend = u'cluster',\n",
    "                color=u'orange', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 3]),#legend = u'cluster',\n",
    "                 color=u'red', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 4]),#legend = u'cluster',\n",
    "                 color=u'blue', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 5]),#legend = u'cluster',\n",
    "                 color=u'yellow', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 6]),#legend = u'cluster',\n",
    "                 color=u'green', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=ColumnDataSource(ClusteredM[ClusteredM.cluster == 7]),#legend = u'cluster',\n",
    "                 color=u'pink', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "\n",
    "# configure visual elements of the plot\n",
    "tsne_plot.title.text_font_size = value(u'16pt')\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# engage!\n",
    "#show(tsne_plot);\n",
    "output_file('PizzaClusteringWebGL.html')\n",
    "#show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the plot for the final clustering model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
